{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c71570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import datasets, utils, callbacks\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe6f92",
   "metadata": {},
   "source": [
    "# 오토인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04519915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패션 mnist 데이터 셋을 통한 오토인코더 구현 실습\n",
    "from tensorflow.keras import datasets\n",
    "(X_train, y_train), (X_test, y_test) = datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf2c111f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    X_train.shape = (60000, 28, 28)\n",
      "    y_tarin.shape = (60000,)\n",
      "    \n",
      "    X_test.shape = (10000, 28, 28)\n",
      "    y_test.shape = (10000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = f'''\n",
    "    X_train.shape = {X_train.shape}\n",
    "    y_tarin.shape = {y_train.shape}\n",
    "    \n",
    "    X_test.shape = {X_test.shape}\n",
    "    y_test.shape = {y_test.shape}\n",
    "'''\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4372c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "def preprocess(imgs):\n",
    "    imgs = imgs.astype('float32') / 255.0 # rgb 값의 표준화\n",
    "    imgs = np.pad(imgs, pad_width = 2, constant_values = 0.0) # pad_width의 두께로 패딩 추가\n",
    "    imgs = np.expand_dims(imgs, axis = -1) # 차원을 확장하는 기능, axis는 어느 열에 차원을 추가할 것인지를 결정하는 것\n",
    "    # axis = -1이면, 맨 마지막 열에 차원 추가\n",
    "    \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "230cd41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    X_train.shape = (60004, 32, 32, 1)\n",
      "    y_tarin.shape = (60000,)\n",
      "    \n",
      "    X_test.shape = (10004, 32, 32, 1)\n",
      "    y_test.shape = (10000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = preprocess(X_train)\n",
    "X_test = preprocess(X_test)\n",
    "\n",
    "p = f'''\n",
    "    X_train.shape = {X_train.shape}\n",
    "    y_tarin.shape = {y_train.shape}\n",
    "    \n",
    "    X_test.shape = {X_test.shape}\n",
    "    y_test.shape = {y_test.shape}\n",
    "'''\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb9a9f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = layers.Input(shape = (32, 32, 1), name = 'encoder_input')\n",
    "\n",
    "x = layers.Conv2D(32, (3, 3), strides = 2, activation = 'relu', padding = 'same')(encoder_input)\n",
    "x = layers.Conv2D(64, (3, 3), strides = 2, activation = 'relu', padding = 'same')(x)\n",
    "x = layers.Conv2D(128, (3, 3), strides = 2, activation = 'relu', padding = 'same')(x)\n",
    "shape_before_flattening = K.int_shape(x)[1:]\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "encoder_output = layers.Dense(2, name = 'encoder_output')(x) # output의 shape = (2, ) 따라서 디코더 input의 shape = (2,)\n",
    "\n",
    "encoder = models.Model(encoder_input, encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d2884dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 32)        320       \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " encoder_output (Dense)      (None, 2)                 4098      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 96,770\n",
      "Trainable params: 96,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "722f35a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = layers.Input(shape = (2,), name = 'decoder_input')\n",
    "\n",
    "x = layers.Dense(np.prod(shape_before_flattening))(decoder_input) # np.prod() - 모든 원소들의 곱을 반환\n",
    "x = layers.Reshape(shape_before_flattening)(x) # 인코더에서 플랫화하기 전의 데이터 형태로 변경\n",
    "x = layers.Conv2DTranspose(128, (3, 3), strides = 2, activation = 'relu', padding = 'same')(x)\n",
    "x = layers.Conv2DTranspose(64, (3, 3), strides = 2, activation = 'relu', padding = 'same')(x)\n",
    "x = layers.Conv2DTranspose(32, (3, 3), strides = 2, activation = 'relu', padding = 'same')(x)\n",
    "\n",
    "decoder_output = layers.Conv2D(1,\n",
    "                               (3, 3),\n",
    "                               strides = 1,\n",
    "                               activation = 'sigmoid',\n",
    "                               padding = 'same',\n",
    "                               name = 'decoder_output')(x)\n",
    "\n",
    "decoder = models.Model(decoder_input, decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f60d6ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              6144      \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 8, 8, 128)        147584    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 16, 16, 64)       73792     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 32, 32, 32)       18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " decoder_output (Conv2D)     (None, 32, 32, 1)         289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 246,273\n",
      "Trainable params: 246,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "13db8544",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = models.Model(encoder_input, decoder(encoder_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d3858911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 32)        320       \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " encoder_output (Dense)      (None, 2)                 4098      \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 32, 32, 1)         246273    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 343,043\n",
      "Trainable params: 343,043\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c712dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer = 'adam', loss = 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7aeb2502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "601/601 [==============================] - 60s 99ms/step - loss: 0.2878 - val_loss: 0.2617\n",
      "Epoch 2/3\n",
      "601/601 [==============================] - 55s 91ms/step - loss: 0.2572 - val_loss: 0.2616\n",
      "Epoch 3/3\n",
      "601/601 [==============================] - 57s 95ms/step - loss: 0.2544 - val_loss: 0.2563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d0ea4bfb80>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_train,\n",
    "                X_train,\n",
    "                epochs = 3,\n",
    "                batch_size = 100,\n",
    "                shuffle = True,\n",
    "                validation_data = (X_test, X_test)) # input 과 output이 같다!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3bfaddbc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/encoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/encoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/decoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/decoder\\assets\n"
     ]
    }
   ],
   "source": [
    "autoencoder.save('./data/autoencoder')\n",
    "encoder.save('./data/encoder')\n",
    "decoder.save('./data/decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "912d9de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "autoencoder = models.load_model('./data/autoencoder')\n",
    "encoder = models.load_model('./data/encoder')\n",
    "decoder = models.load_model('./data/decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ff120b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# 이미지 재구성\n",
    "example_images = X_test[:500]\n",
    "predictions = autoencoder.predict(example_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328ce0e2",
   "metadata": {},
   "source": [
    "### 잠재 공간 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164a1c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "embeddings = encoder.predict(example_images)\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.scatter(embeddings[:, 0], embeddings[:, 1], c = 'black', alpha = 0.5, s = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_labels = y_test[:500]\n",
    "\n",
    "figsize = 8\n",
    "plt.figure(figsize = (figsize, figsize))\n",
    "plt.scatter(embeddings[:, 0],\n",
    "            embeddings[:, 1],\n",
    "            cmap = 'rainbow',\n",
    "            c = example_labels,\n",
    "            alpha = 0.6,\n",
    "            s = 3)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a26d05",
   "metadata": {},
   "source": [
    "### 새로운 이미지 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b86b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mins, maxs = np.min(embeddings, axis = 0), np.max(embeddings, axis = 0)\n",
    "sample = np.random.uniform(mins, maxs, size = (18, 2))\n",
    "# 총 18 * 2의 숫자를 뽑는데, (x, y)의 형태로 18개를 뽑는 것\n",
    "\n",
    "reconstructions = decoder.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4325bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addae979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f08416f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[76.91425136,  1.56879972],\n",
       "       [ 1.04864136, 33.18426395],\n",
       "       [36.8614864 , 56.07546463],\n",
       "       [82.12636056, 89.25478635],\n",
       "       [43.49610641, 21.38876741],\n",
       "       [94.13457316, 27.33449787],\n",
       "       [ 7.22842278, 52.84174938],\n",
       "       [44.01081147, 47.36034597],\n",
       "       [ 7.95839605,  0.27944804],\n",
       "       [58.98430633,  2.35548157]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(0, 100, size = (10, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
